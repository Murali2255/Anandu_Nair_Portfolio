<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Anandu Sanil Nair</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top">
    <div class="container d-flex align-items-center justify-content-between">

      <h1 class="logo"><a href="index.html">Anandu Sanil Nair</a></h1>
      <!-- Uncomment below if you prefer to use an image logo -->
      <!-- <a href="index.html" class="logo"><img src="assets/img/logo.png" alt="" class="img-fluid"></a>-->

      <nav id="navbar" class="navbar">
        <ul>
          <li><a class="nav-link scrollto active" href="#hero">Home</a></li>
          <li><a class="nav-link scrollto" href="#about">About</a></li>
          <li><a class="nav-link scrollto" href="#skills">Skills</a></li>
          <li><a class="nav-link scrollto " href="#experience">Experience</a></li>
          <li><a class="nav-link scrollto " href="#education">Education</a></li>

        
          </li>
          <li><a class="nav-link scrollto" href="#contact">Contact</a></li>
          <li><a class="nav-link scrollto" href="./assets/resume/AnanduNair_Data Engineer.pdf" target="_blank">Resume</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->

    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <div id="hero" class="hero route bg-image" style="background-color: #7fffd4;">
    <div class="overlay-itro"></div>
    <div class="hero-content display-table">
      <div class="table-cell">
        <div class="container">
          <!--<p class="display-6 color-d">Hello, world!</p>-->
          <h1 class="hero-title mb-4">I am Anandu Sanil Nair</h1>
          <p class="hero-subtitle"><span class="typed" data-typed-items="Data Engineer"></span></p>
          <!-- <p class="pt-3"><a class="btn btn-primary btn js-scroll px-4" href="#about" role="button">Learn More</a></p> -->
        </div>
      </div>
    </div>
  </div><!-- End Hero Section -->

  <main id="main">

    <!-- ======= About Section ======= -->
    <section id="about" class="about-mf sect-pt4 route">
      <div class="container">
        <div class="row">
          <div class="col-sm-12">
           
            <div class="box-shadow-full">
              <div class="title-box text-center">
                <h3 class="title-a">
                  About
                </h3>
                <div class="line-mf"></div>
                </div>
              <div class="row">
                <div class="col-lg-4 col-md-12">
                 
                      <div class="about-img">
                        <img src="assets/img/Anandu_pic.jpg" class="img-fluid rounded b-shadow-a" alt=""  width="300px" height="300px">
                    </div>
                </div>
                <div class="col-lg-8 col-md-12">
                  <div class="about-me pt-md-0">
                   
                    <p >
                      Data Engineer with experience in building data solutions using SQL Server, MSBI AWS Azure Cloud.
                    </p>
                    <p>
                      Experience in Azure Cloud, Azure Data Factory, Azure Data Lake Storage, Azure Synapse Analytics, Azure
Analytical services, Azure Cosmos NO SQL DB, and Data bricks
                    </p>
                    <p>
                      Well versed with Agile with Scrum, Waterfall Model and Test-driven Development (TDD) methodologies
                    </p>
                    <p>
                      Extensive working experience in implementation and operations of data governance, data strategy, data 
management and solutions.
                    </p>
                    <p>
                      Experience on Migrating SQL database to Azure Data Lake, Azure Data Lake Analytics, Azure SQL Database, Data 
Bricks and Azure SQL Data warehouse and Controlling and granting database access and Migrating On premise 
databases to Azure Data lake store using Azure Data factory.

                    </p>
                    <p>
                      Experience with an in-depth level of understanding in the strategy and practical implementation of AWS CloudSpecific technologies including EC2, EBS, S3, VPC, RDS, SES, ELB, EMR, ECS, Cloud Front, Cloud Formation, Elastic 
Cache, Cloud Watch, Red Shift, Lambda, SNS, Dynamo DB, Kinesis.
                    </p>
                  </div>
                </div>
              </div>
              <div class="row">
                
                <div class="col-md-12">
                  <div class="about-me pt-md-0">
                    
                    <p >
                      Hands on experience on AWS cloud services (VPC, EC2, S3, RDS, Redshift, Data Pipeline, EMR, DynamoDB, 
                      Workspaces, Lambda, Kinesis, RDS, SNS, SQS)
                    </p>
                    <p>
                      Proficient in SQLite, MySQL and SQL databases with Python.

                    </p>
                    <p>
                      Practical understanding of the Data modeling (Dimensional & Relational) concepts like Star - Schema modeling, 
Snowflake Schema Modeling, Fact and Dimension tables
                    </p>
                    <p>
                      Experience in handling python and spark context when writing PySpark programs for ETL.

                    </p>
                    <p>
                      Strong knowledge in data visualization using Power BI and Tableau.

                    </p>
                    <p>
                      Hands in experience on NoSQL database like Snowflake, HBase, Cassandra and MongoDB.
                    </p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section><!-- End About Section -->

    <!-- ======= Services Section ======= -->
    <section id="skills" class="services-mf pt-5 route">
      <div class="container">
        <div class="row">
          <div class="col-sm-12">
            <div class="title-box text-center">
              <h3 class="title-a">
                Skills
              </h3>
              <div class="line-mf"></div>
              <p class="subtitle-a">
                Reliable Data Engineer with over 8+ years of experience in data warehousing, data engineering, feature engineering, 
big data, ETL/ELT, and business intelligence. As a Data architect and engineer, specialize in AWS, Azure frameworks, 
Cloudera, Hadoop Eco system, Python Spark/Py Spark/Scala, Data bricks, Hive, Redshift, Snowflake, relational databases, 
tools like Tableau, Airflow, DBT, Presto/Athena, Data DevOps Frameworks/Pipelines with strong Programming/Scripting 
Skills. Expert data sets and conducting performance tuning.
              </p>
              
            </div>
          </div>
        </div>
        <div class="row">
          <div class="col-md-12">
            <ul>
              <li class="textList">
                <strong>Hadoop Components </strong>
                HDFS, Hue, MapReduce, PIG, Hive, HCatalog, Hbase, Sqoop, 
Impala, Zookeeper, Flume, Kafka, Yarn, Cloudera Manager, 
Kerberos, pyspark
              </li>
              <br>
              <li class="textList">
                <strong>Spark Components</strong>
                Apache Spark, Data Frames, Spark SQL, Spark, YARN, Pair 
RDDs
              </li>
              <br>

              <li class="textList"><strong>Web Technologies / Other components </strong>
                J2EE, XML, Log4j, HTML, XML, CSS, JavaScript</li>
              <br>

              <li class="textList"><strong>Server Side Scripting
                </strong>
                UNIX Shell, Power Shell Python Scripting (Boto3)</li>
              <br>
              <li class="textList"><strong>Databases </strong>
                Oracle, Microsoft SQL Server, MySQL, DB2, Teradata, 
snowflake
              </li>
              <br>
              <li class="textList">
                <strong>Programming Languages</strong>
                Java, Scala, Impala, Python
              </li> <br>
              <li class="textList">
                <strong>Web Servers</strong>
                Apache Tomcat, WebLogic

              </li> <br>
              <li class="textList">
                <strong>IDE</strong>
                Eclipse, Dreamweaver


              </li> <br>
              <li class="textList">
                <strong>OS/Platforms</strong>
                Windows, Linux (All major distributions), Unix, CENTOS


              </li> <br>
              <li class="textList">
                <strong>NoSQL Databases</strong>
                Hbase, MongoDB

              </li> <br>
              <li class="textList">
                <strong>Methodologies</strong>
                Agile (Scrum), Waterfall, UML, Design Patterns, SDLC.

              </li> <br>
              <li class="textList">
                <strong>Currently Exploring</strong>
                Apache, Flink, Drill, Tachyon.

              </li> <br>

              <li class="textList">
                <strong>Cloud Services</strong>
                AWS, Azure

              </li> <br>
              <li class="textList">
                <strong>AWS Services</strong>
                S3, EC2, EMR, Redshift, RDS, Glue, Lambda, Kinesis, SNS, 
SQS,AMI, IAM, Cloud formation

              </li> <br>
              <li class="textList">
                <strong>Azure</strong>
                Azure Data Factory / ETL / ELT / SSIS , Azure Data Lake Storage, Azure Databricks

              </li> <br>
              <li class="textList">
                <strong>ETL Tools </strong>
                Talend Open Studio & Talend Enterprise Platform


              </li> <br>
              <li class="textList">
                <strong>Machine Learning </strong>
                Regression Methods: Linear, Multiple, 
Polynomial, Decision trees. 


              </li> <br>
              
            </ul>
          </div>
        </div>
      </div>
    </section><!-- End Services Section -->

   

    <!-- ======= Portfolio Section ======= -->
    <section id="experience" class="resume">
      <div class="container">
        <div class="row">
          <div class="col-sm-12">
            <div class="title-box text-center">
              <h3 class="title-a">
                Experience
              </h3>
             
              <div class="line-mf"></div>
            </div>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-6">
           
            <div class="resume-item">
              <h4>Senior Data Engineer</h4>
              <h5>Mar 2021 – till date</h5>
              <p><em>AbbVie (Chicago, Illinois)</em></p>
              <p>
              <ul>
                <li class="textList">Understood the current Production state of application and determine the impact of new implementation on 
                  existing business processes.</li>
                <li class="textList">Part of the Agile Team and work on weeks sprints, daily sprint Status, sprint demo preparation and stakeholder 
                  demo and signoff </li>
                <li class="textList">Worked collaboratively with all levels of business stakeholders to architect, implement and test Big Data based 
                  analytical solution from disparate sources.</li>
                <li class="textList">Proposed architectures considering cost/spend in Azure and develop recommendations to right-size data 
                  infrastructure.
                  </li>
                  <li class="textList">Traced and catalogue data processes, transformation logic and manual adjustments to identify data governance 
                    issues</li>
                  <li class="textList">Participated in the Data Governance working group sessions to create Data Governance Policies.</li>
                  <li class="textList">Analyzed, designed and built Modern data solutions using Azure PaaS service to support visualization of data.</li>
                  <li class="textList"><strong>Environment : </strong> Hadoop 3.3, HDFS, Spark 3.2, PySpark, Azure, Power BI, Sqoop, Zookeeper, ADF, Hive, Sqoop, Maven, 
                    JSON, Kafka</li>
              </ul>
              </p>
            </div>
            <div class="resume-item">
              <h4>Data Engineer</h4>
              <h5>May 2019 - Feb 2021</h5>
              <p><em>Baker Hughes (Houston, TX) </em></p>
              <p>
              <ul>
                <li class="textList">Designed and built Spark/Pyspark based ETL pipelines for migration of credit card transactions, account and 
                  customer data into enterprise Hadoop Data Lake. Developed strategies in handling large datasets using partitions, 
                  Spark SQL, broadcast joins and performance tuning. </li>
                <li class="textList">Built and implemented performant data pipelines using Apache Spark on AWS EMR. Performed maintenance of 
                  data integration programs into Hadoop and RDBMS environments from both structured and semi- structured data 
                  source systems.</li>
                <li class="textList">Developed a 16-node cluster in designing the Data Lake with the Hortonworks distribution</li>
                <li class="textList">Developed performance tuning on existing Hive queries and UDF’s to analyze the data. Used Pig to analyze datasets 
                  and perform transformation according to requirements. </li>
                  <li class="textList">
                    Supervised on data profiling and data validation to ensure the accuracy of the data between the source and the 
                    target systems. Performed job scheduling and monitoring using Auto sys and quality testing using ALM
                  </li>
                  <li class="textList">
                    Worked on building of Tableau desktop reports and dashboards to report customer data.
                  </li>
                  <li class="textList">
                    Built and published customized interactive Tableau reports and dashboards along with data refresh scheduling using 
                    Tableau Desktop.
                  </li>
                  <li class="textList">
                    Developed store procedures/views in Snowflake and used in AWS glue for loading Dimensions and Facts.

                  </li>
                  <li class="textList">
                    <strong>Environment : </strong> AWS, Hadoop, Python, Pyspark, SQL, Snowflake, Data bricks/Delta Lake, AWS S3, AWS Athena and AWS 
EMR.
                  </li>
                  
              </ul>
              </p>
            </div>
            <div class="resume-item">
              <h4>Data Engineer</h4>
              <h5>Apr 2018 - Apr 2019
              </h5>
              <p><em>Carbon Health (San Francisco, CA)  </em></p>
              <p>
              <ul>
                <li class="textList">Responsible for building scalable distributed data solutions using Hadoop </li>
                <li class="textList">Involved in Agile Development process (Scrum and Sprint planning).
                </li>
                <li class="textList">Handled Hadoop cluster installations in Windows environment.</li>
                <li class="textList">Migrated data warehouses to Snowflake Data warehouse.
                </li>
                  <li class="textList">
                    Defined virtual warehouse sizing for Snowflake for different type of workloads.
                  </li>
                  <li class="textList">
                    Extracted data from data lakes, EDW to relational databases for analyzing and getting more meaningful insights 
using SQL Queries and PySpark
                  </li>
                  <li class="textList">
                    Designed, developed and did maintenance of data integration programs in a Hadoop and RDBMS environment 
                    with both traditional and non-traditional source systems
                  </li>
                  <li class="textList">
                    Developed MapReduce programs to parse the raw data, populate staging tables and store the refined data in 
                    partitioned tables in the EDW.

                  </li>
                  <li class="textList">
                    <strong>Environment : </strong>  Hadoop 3.3, Big Query, Big Table, Spark 3.0, Sqoop 1.4.7, ETL, HDFS, Snowflake DW, Oracle SQL, 
                    MapReduce, Kafka 2.8 and Agile process
EMR.
                  </li>
                  
              </ul>
              </p>
            </div>
          </div>
          <div class="col-lg-6">
           
            <div class="resume-item">
              <h4>Data Engineer</h4>
              <h5>Jan 2017 - Mar 2018
              </h5>
              <p><em>Fortinet (Sunnyvale, CA)</em></p>
              <p>
              <ul>
                <li class="textList">Designed and deployed scalable, highly available, and fault tolerant systems on Azure.</li>
                <li class="textList">Led the estimation, review the estimates, identify the complexities and communicate to all the stakeholders.
                   </li>
                <li class="textList">Involved in complete SDLC life cycle of big data project that includes requirement analysis, design, coding, 
                  testing and production.</li>
                <li class="textList">Defined the business objectives comprehensively through discussions with business stakeholders, functional 
                  analysts and participating in requirement collection sessions.
                  </li>
                  <li class="textList">
                    Implemented end-to-end systems for Data Analytics, Data Automation and integrated with custom visualization 
                    tools. Migrated on-primes environment on Cloud using MS Azure.</li>
                    <li class="textList">Designed the business requirement collection approach based on the project scope and SDLC (Agile) 
                      methodology. Moved data to Azure Data Lake to Azure data warehouse using PolyBase</li>
                      <li class="textList">Created external tables in ADW with 4 compute nodes and scheduled.
                      </li>
                      <li class="textList">Extensively used Agile Method for daily scrum to discuss the project related information.</li>
                      <li class="textList">
                      <strong>Environment : </strong>Spark 2.8, Kafka 2.6.2, Apache Airflow 1.10, Azure SQL DB, Azure DW, Azure Data Lake, Azure Data 
                      factory, Python 3.7, XML, Azure Databricks, T-SQL and Agile process.
                    </li>
              </ul>
              </p>
            </div>
            <div class="resume-item">
              <h4>Data Engineer</h4>
              <h5>May 2016 - Dec 2017</h5>
              <p><em>Edward Jones (St. Louis, MO)</em></p>
              <p>
              <ul>
                <li class="textList">Collaborated with data architects to understand and get the data requirements for the project model as per the 
                  business.</li>
                <li class="textList">Developed Python script to hit Rabbit MQ API and extract data in JSON format and load into Spark RDD. Developed Spark program using PySpark, to handle Streaming data and load data Azure Events Hubs.
                  </li>
                <li class="textList"> Developed Talend ingestion frameworks to ingest between Teradata, MSSQL, HDFS, Azure Blobs and Azure Data 
                  warehouse.</li>
                <li class="textList">Extract Transform and Load data from Sources Systems to Azure Data Storage service using a combination of 
                  Azure Data Factory and ingest data Azure Blob storage and processing the data in Azure Databricks.</li>
                
                <li class="textList">
                  Responsible for estimating the cluster size, monitoring and troubleshooting of the Spark Databricks cluster. 
                  Loaded data into Azure SQL database using Azure Databricks.
                </li>
                <li class="textList">Processed real time structured data using stream analytics in ADW in order to merge with the data that is being 
                  ingested from Hadoop to ADW.</li>
                  <li class="textList">Ingested structured data from MySQL, SQL Server to HDFS as incremental import using Talend jobs. These 
                    imports are scheduled to run in a periodic manner.</li>
                <li class="textList">
                  <strong>Environment : </strong> Python, HDFS, PySpark, Hive, Rabbit MQ, SQL, Azure (ADF, Blobs, SQL Data warehouse), MSSQL, Teradata, 
                  Power BI.
                </li>
              </ul>
              </p>
            </div>
            <div class="resume-item">
              <h4>Data Analyst</h4>
              <h5>July 2014 - Dec 2015</h5>
              <p><em>Wissen Infotech (India)</em></p>
              <p>
              <ul>
                <li class="textList">Responsible for gathering data migration requirements. Identified problematic areas and conduct research to 
                  determine the best course of action to correct the data.</li>
                <li class="textList">Analyzed problem and solved issues with current and planned systems as they relate to the integration and 
                  management of order data. Involved in Data Mapping activities for the data warehouse.
                  </li>
                <li class="textList">Analyzed reports of data duplicates or other errors to provide ongoing appropriate inter-departmental 
                  communication and monthly or daily data reports.
                  </li>
                <li class="textList">Monitor for timely and accurate completion of select data elements.
                </li>
                <li class="textList">
                  Collected, analyzed and interpreted complex data for reporting and/or performance trend analysis.
                </li>
                
                  <li class="textList">Compared data with original source documents and validate Data accuracy.</li>
                <li class="textList">
                  Involved in analyzing and adding new features of Oracle 10g like DBMS_SHEDULER, Create Directory, Data 
                  pump, CONNECT_BY_ROOT in existing Oracle 10g application. </li>
                  <li class="textList">
                    <strong>Environment : </strong> UNIX, Shell Scripting, XML Files, XSD, XML, SAS, PL/SQL, Oracle 10g, Erwin 9.5, Autosys
                  </li>
              </ul>
              </p>
            </div>
          </div>
        </div>

      </div>
    </section><!-- End Portfolio Section -->

    <section id="education" class="resume">
      <div class="container">
        <div class="row">
          <div class="col-sm-12">
            <div class="title-box text-center">
              <h3 class="title-a">
                Education
              </h3>
              
              <div class="line-mf"></div>
            </div>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-6">
            <!-- <h3 class="resume-title">Education</h3> -->
            <div class="resume-item">
              <h4>Masters in Data Science, University of Buffalo</h4>
              <h5> 2017</h5>
              <p><em> The State University of New York,NY</em></p>
              
            </div>


          </div>
          <div class="col-lg-6">
            <!-- <h3 class="resume-title">Education</h3> -->

            <div class="resume-item">
              <h4>Bachelors in Computer Science</h4>
              <h5>2014</h5>
              <p><em> Pillai College of Engineering,Ind</em></p>
              
            </div>
          </div>
        </div>
      </div>
    </section>

   

    <!-- ======= Contact Section ======= -->
    <section id="contact" class="paralax-mf footer-paralax bg-image sect-mt4 route" style="background-image: url(assets/img/overlay-bg.jpg)">
      <div class="overlay-mf"></div>
      <div class="container">
        <div class="row">
          <div class="col-sm-12">
            <div class="contact-mf">
              <div id="contact" class="box-shadow-full">
                <div class="row">
                  <div class="col-md-6">
                    <div class="row">
                      <div class="col-lg-6 col-md-12 ">
                        <div class="info-box contact-details mt-4">
                         
                          <h3>Email Me</h3>
                          <p>
        
                            <a href="mailto:anandunair284@gmail.com" class="linkin-class"
                              target="_blank">anandunair284@gmail.com</a>
                          </p>
                        </div>
                      </div>
                      <div class="col-lg-6 col-md-12">
                        <div class="info-box contact-details mt-4">
                          
                          <h3>Linked In</h3>
                          <p><a class="linkin-class"
                              href="https://www.linkedin.com/in/anandu-nair-394699265/"
                              target="_blank">linkedin.com/in/anandu-nair-394699265/
                            </a></p>
                        </div>
                      </div>
        
                    </div>
                  </div>
                  <div class="col-md-6">
                    <div class="row">
                      <div class="col-lg-6 col-md-12">
                        <div class="info-box contact-details mt-4">
                          
                          <h3>Call Me</h3>
                          <p> 716 939 9370</p>
                        </div>
                      </div>
                      <div class="col-lg-6 col-md-12">
                        <div class="info-box contact-details mt-4">
                          
                          <h3>Location</h3>
                          <p>Buffalo, NY</p>
                        </div>
                      </div>
        
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End Contact Section -->

  </main><!-- End #main -->

  <div id="preloader"></div>
  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/typed.js/typed.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>